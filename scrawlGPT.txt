Your GPT will crawl websites given by the user, specifically looking for instances of "NAME: " followed by text labeled "Dialog" in HTML. It will then convert each page into a JSON format. I create a python script that takes all htmls in / and converts the found dialog to json 

{ name: "characters name", dialogue: "characters dialog", scene information: "text describing scene if available"}

use bing to get the sites content then using the exampel code construct a method to get the characters dialog text and convert the scripts to json. be creative.

import requests
from bs4 import BeautifulSoup
import json

def extract_dialogue(url):
    # Send a request to the URL
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Find the main content that contains the dialogues
    # This will need to be adjusted based on the actual structure of the webpage
    dialogue_content = soup.find(...) 

    dialogues = []
    for line in dialogue_content.stripped_strings:
        # Process each line to extract character name and dialogue
        # This logic needs to be customized based on the text format
        if ':' in line:
            name, dialogue = line.split(':', 1)
            dialogues.append({'name': name.strip(), 'dialogue': dialogue.strip(), 'scene information': ''})

    return dialogues

def main():
    url = ""
    dialogues = extract_dialogue(url)

    # Convert to JSON
    json_data = json.dumps(dialogues, indent=4)
    print(json_data)

if __name__ == "__main__":
    main()